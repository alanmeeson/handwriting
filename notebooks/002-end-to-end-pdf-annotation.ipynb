{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1649f5c-f492-4760-a0fd-b4f80d785fa6",
   "metadata": {},
   "source": [
    "# End to End PDF Annotation Demo\n",
    "\n",
    "**Author:** Alan Meeson <alan@carefullycalculated.co.uk>\n",
    "\n",
    "**Date:** 2023-04-16\n",
    "\n",
    "This notebook applies the custom trained handwriting region detection model (based on Fast R CNN), and the TROCR handwriting OCR model to detect and parse handwriting.\n",
    "\n",
    "In addition we have Non-Max Suppression, and a custom removal of sub-set regions to identify the key text regions to process.\n",
    "\n",
    "The output is a json file of regions coordinates and parsed text, also an annotated PDF with invisitext of the parsed text overlaid ontop of the original handwriting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5cd9f-ae44-40a2-a38f-f839feee113c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c50cd2-5072-4127-a4d3-e9df860a31cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import fitz\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b010e2e-8bff-412a-b865-6d23030358d1",
   "metadata": {},
   "source": [
    "## Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9feccd-3d22-4d67-8321-a90df1b6aa87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_region_model(\n",
    "    num_classes: int = 1, weights_file: str = None, for_eval: bool = True\n",
    ") -> torch.nn.Module:\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    if weights_file:\n",
    "        device = None\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # We're using GPU\n",
    "            device = torch.device(\"cuda\")\n",
    "            model.load_state_dict(torch.load(weights_file))\n",
    "            model.to(device)\n",
    "        else:\n",
    "            # We're using CPU\n",
    "            device = torch.device('cpu')\n",
    "            model.load_state_dict(torch.load(weights_file, map_location=device))\n",
    "\n",
    "        if for_eval:\n",
    "            model.eval()\n",
    "\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4ea87-7497-4f89-9c75-7f125af06a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nms_pytorch(boxes : torch.tensor, scores: torch.tensor, thresh_iou : float=0.5) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Apply non-maximum suppression to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the image [num_boxes, 4]\n",
    "        scores: (tensor) The score of each box [num_boxes, 1]\n",
    "        thresh_iou: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "    Returns:\n",
    "        A list of filtered boxes, Shape: [ , 4]\n",
    "    \"\"\"\n",
    " \n",
    "    #TODO: replace this with the torchvision version\n",
    "    # https://pytorch.org/vision/master/generated/torchvision.ops.nms.html\n",
    "    \n",
    "    # we extract coordinates for every \n",
    "    # prediction box present in P\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    " \n",
    "    # calculate area of every block in P\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "     \n",
    "    # sort the prediction boxes in P\n",
    "    # according to their confidence scores\n",
    "    order = scores.argsort()\n",
    " \n",
    "    # initialise an empty list for \n",
    "    # filtered prediction boxes\n",
    "    keep = []\n",
    "     \n",
    " \n",
    "    while len(order) > 0:\n",
    "         \n",
    "        # extract the index of the \n",
    "        # prediction with highest score\n",
    "        # we call this prediction S\n",
    "        idx = order[-1]\n",
    " \n",
    "        # push S in filtered predictions list\n",
    "        keep.append([x1[idx], y1[idx], x2[idx], y2[idx], scores[idx]])\n",
    " \n",
    "        # remove S from P\n",
    "        order = order[:-1]\n",
    " \n",
    "        # sanity check\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "         \n",
    "        # select coordinates of BBoxes according to \n",
    "        # the indices in order\n",
    "        xx1 = torch.index_select(x1,dim = 0, index = order)\n",
    "        xx2 = torch.index_select(x2,dim = 0, index = order)\n",
    "        yy1 = torch.index_select(y1,dim = 0, index = order)\n",
    "        yy2 = torch.index_select(y2,dim = 0, index = order)\n",
    " \n",
    "        # find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    " \n",
    "        # find height and width of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "         \n",
    "        # take max with 0.0 to avoid negative w and h\n",
    "        # due to non-overlapping boxes\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    " \n",
    "        # find the intersection area\n",
    "        inter = w*h\n",
    " \n",
    "        # find the areas of BBoxes according the indices in order\n",
    "        rem_areas = torch.index_select(areas, dim = 0, index = order) \n",
    " \n",
    "        # find the union of every prediction T in P\n",
    "        # with the prediction S\n",
    "        # Note that areas[idx] represents area of S\n",
    "        union = (rem_areas - inter) + areas[idx]\n",
    "         \n",
    "        # find the IoU of every prediction in P with S\n",
    "        IoU = inter / union\n",
    " \n",
    "        # keep the boxes with IoU less than thresh_iou\n",
    "        mask = IoU < thresh_iou\n",
    "        order = order[mask]\n",
    "     \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb27bb9-45f7-4a51-acec-34509b74a86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sss_pytorch(boxes : torch.tensor, thresh_overlap : float=0.9) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Apply sub-set suppression to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    This specifically removes boxes which are (almost) entirely contained within \n",
    "    another box.\n",
    "    \n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the image and scores [num_boxes, 5]\n",
    "        thresh_overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "    Returns:\n",
    "        A list of filtered boxes, Shape: [ , 5]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Can we tidy this up?\n",
    "\n",
    "    # we extract coordinates for every \n",
    "    # prediction box present in P\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    " \n",
    "    # calculate area of every block in P\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "     \n",
    "    # sort the prediction boxes in P\n",
    "    # according to their area\n",
    "    order = areas.argsort()\n",
    " \n",
    "    # initialise an empty list for \n",
    "    # filtered prediction boxes\n",
    "    keep = torch.as_tensor([False] * boxes.shape[0])\n",
    "    \n",
    "    if len(order) > 0:\n",
    "        idx = order[-1]\n",
    "        keep[idx] = True\n",
    "        order = order[:-1]\n",
    " \n",
    "    while len(order) > 0:\n",
    "         \n",
    "        # extract the index of the \n",
    "        # prediction with highest score\n",
    "        # we call this prediction S\n",
    "        idx = order[-1]\n",
    "        order = order[:-1]\n",
    " \n",
    "        # sanity check\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "         \n",
    "        # select coordinates of BBoxes according to \n",
    "        # the indices in order\n",
    "        xx1 = x1[keep]\n",
    "        xx2 = x2[keep]\n",
    "        yy1 = y1[keep]\n",
    "        yy2 = y2[keep]\n",
    " \n",
    "        # find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    " \n",
    "        # find height and width of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "         \n",
    "        # take max with 0.0 to avoid negative w and h\n",
    "        # due to non-overlapping boxes\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    " \n",
    "        # find the intersection area\n",
    "        inter = w*h\n",
    " \n",
    "        overlap = inter / areas[idx]\n",
    "\n",
    "        keep[idx] = all(overlap < thresh_overlap)\n",
    "     \n",
    "    return boxes[keep, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1bad45-24ed-465e-b61b-37002f277c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_image(model, device, image: Image):\n",
    "    \"\"\"Applies the text region detection to an Image.\n",
    "    \n",
    "    Args:\n",
    "        model: the model to apply\n",
    "        device: the device the model is on, so we can send image there.\n",
    "        image: the PIL image to process\n",
    "    Returns:\n",
    "        A list of dicts containing class label, confidence score and coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: refactor,  device should probably not be in here.\n",
    "    #  Perhaps wrap this up in a class?\n",
    "    \n",
    "    # Prepare image and apply model\n",
    "    pil_image = F.pil_to_tensor(image) / 255\n",
    "    cuda_pil_image = pil_image.to(device)\n",
    "    result = model([cuda_pil_image])\n",
    "    \n",
    "    # Apply Non-Max Suppression\n",
    "    target_class = 1\n",
    "    mask = result[0]['labels'] == target_class\n",
    "    target_boxes = result[0]['boxes'][mask,:]\n",
    "    target_scores = result[0]['scores'][mask]\n",
    "    if target_boxes.shape[0] > 0:\n",
    "        filtered_boxes = nms_pytorch(target_boxes, target_scores, thresh_iou=0.25)\n",
    "        filtered_boxes = torch.as_tensor(filtered_boxes)\n",
    "        filtered_boxes = sss_pytorch(filtered_boxes, thresh_overlap = 0.9)\n",
    "    else:\n",
    "        filtered_boxes = target_boxes\n",
    "    \n",
    "    num_boxes = filtered_boxes.shape[0]\n",
    "    \n",
    "    # reformat results\n",
    "    #label = result[0]['labels'][mask]\n",
    "    results = [\n",
    "        {\n",
    "            'label': target_class,\n",
    "            'score': float(filtered_boxes[idx, 4]),\n",
    "            'box': {\n",
    "                'x1': int(filtered_boxes[idx,0]),\n",
    "                'y1': int(filtered_boxes[idx,1]),\n",
    "                'x2': int(filtered_boxes[idx,2]),\n",
    "                'y2': int(filtered_boxes[idx,3])\n",
    "            }\n",
    "        }\n",
    "        for idx in range(num_boxes)\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ff602-00be-4d91-b40e-bb2d190315a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_ocr(ocr_model, processor, image, boxes, device=None):\n",
    "    \"\"\"Applies the TROCR model to the image.\n",
    "    \n",
    "    Args:\n",
    "        ocr_model - the trocr model to apply\n",
    "        processor - required preprocessing steps\n",
    "        image - the image to process\n",
    "        boxes - the list of dicts describing the identified text regions\n",
    "        device - the device the model is on\n",
    "    Return:\n",
    "        The list of boxes, with an added field for the parsed text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: refactor\n",
    "    \n",
    "    for box in boxes:\n",
    "        x1 = box['box']['x1']\n",
    "        y1 = box['box']['y1']\n",
    "        x2 = box['box']['x2']\n",
    "        y2 = box['box']['y2']\n",
    "        region_of_interest = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        pixel_values = processor(images=region_of_interest, return_tensors=\"pt\").pixel_values\n",
    "        if device:\n",
    "            pixel_values = pixel_values.to(device)\n",
    "        \n",
    "        generated_ids = ocr_model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        box['text'] = generated_text\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18c416-0ca9-454a-8c93-19232b2c13d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_text_annotations(page, results):\n",
    "    \"\"\"Annotates a PDF page with the parsed text.\n",
    "    \n",
    "    Args:\n",
    "        page - the PyMuPDF page to annotate\n",
    "        results - the boxes and text to add\n",
    "    \"\"\"\n",
    "    tw = fitz.TextWriter(page.rect)  # need the intended page's size here\n",
    "\n",
    "    font = fitz.Font(\"helv\")\n",
    "    \n",
    "    for box in results:\n",
    "        # Note: the /2 is to account for the fact that the parsed\n",
    "        # boxes were found from a page with zoom=2.\n",
    "        # TODO: fix this nasty hack.\n",
    "        pos = (box['box']['x1']/2, box['box']['y2']/2)\n",
    "        text = box['text'][0]\n",
    "        fontsize = (box['box']['y2'] - box['box']['y1']) / (2*2)\n",
    "        \n",
    "        # for each text piece (a word, a string, a character, ... everything goes)\n",
    "        tw.append(\n",
    "            pos,  # the insertion point\n",
    "            text,\n",
    "            font=font,  # a fitz.Font(...) object\n",
    "            fontsize=fontsize,\n",
    "        )\n",
    "    \n",
    "    # ... repeat the above with arbitrary other fonts / fontsizes, when done:\n",
    "    tw.write_text(page, render_mode=3)  # write the whole text writer as hidden (render mode 3) text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f527ce9-6f25-4858-92fd-c0907fd9dc6c",
   "metadata": {},
   "source": [
    "## Declare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b409a0-a2ab-472f-ad12-6cb9453907a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = '../data/raw/demo.pdf'\n",
    "model_file = '../models/handwriting_region.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34077670-fd46-471e-a982-1f82753e58ad",
   "metadata": {},
   "source": [
    "## Load data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9397982-aaea-4b22-b32a-bfd36de2289e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: download seperately to allow no-internet deploy\n",
    "td_model, device = get_text_region_model(num_classes = 2, weights_file = model_file)\n",
    "ocr_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "ocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "ocr_model.to(device)\n",
    "ocr_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4511b9-1aa7-455c-875a-9d0206c83b59",
   "metadata": {},
   "source": [
    "## Apply the models as a Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f288c76-5a3a-4462-98ea-67dcfc19a47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_doc = fitz.open(input_file)\n",
    "page = input_doc[0]\n",
    "zoom = 2    # zoom factor\n",
    "mat = fitz.Matrix(zoom, zoom)\n",
    "pix = page.get_pixmap(matrix=mat)\n",
    "image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "results = parse_image(td_model, device, image)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf928e-95fb-46d3-95e8-80b6c520f7f4",
   "metadata": {},
   "source": [
    "### Plot the detected text regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d51d6-4112-416d-a824-58f3e7029a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_boxes = len(results)\n",
    "\n",
    "img = np.array(image)\n",
    "\n",
    "for box in results:\n",
    "    x1 = box['box']['x1']\n",
    "    y1 = box['box']['y1']\n",
    "    x2 = box['box']['x2']\n",
    "    y2 = box['box']['y2']\n",
    "    \n",
    "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color=(0,0,255),thickness=3)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835ba54-50a0-4b3c-82ae-a9d15cca31a3",
   "metadata": {},
   "source": [
    "### Apply the OCR step and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384e99e-c300-4c4d-8a80-e01de2b04031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for box in results:\n",
    "    x1 = box['box']['x1']\n",
    "    y1 = box['box']['y1']\n",
    "    x2 = box['box']['x2']\n",
    "    y2 = box['box']['y2']\n",
    "    region_of_interest = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    pixel_values = ocr_processor(images=region_of_interest, return_tensors=\"pt\").pixel_values\n",
    "    cuda_pix_values = pixel_values.to(device)\n",
    "    generated_ids = ocr_model.generate(cuda_pix_values)\n",
    "    generated_text = ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    box['text'] = generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16926cd9-8646-4ffb-b431-9ef28f6675d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples = len(results)\n",
    "f, axarr = plt.subplots(len(results),1, figsize=(8,num_samples))\n",
    "\n",
    "sorted_index = np.argsort([box['box']['y1'] for box in results])\n",
    "for ax, idx in enumerate(sorted_index):\n",
    "    box = results[idx]\n",
    "    x1 = box['box']['x1']\n",
    "    y1 = box['box']['y1']\n",
    "    x2 = box['box']['x2']\n",
    "    y2 = box['box']['y2']\n",
    "    region_of_interest = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    axarr[ax].title.set_text(box['text'])\n",
    "    axarr[ax].imshow(region_of_interest)\n",
    "    axarr[ax].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42f6ad-950c-449d-bf77-b3335d256785",
   "metadata": {},
   "source": [
    "## Apply to a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e5ae9-0cc1-4388-b00a-c27c11091efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = '../data/output'\n",
    "output_pdf = True\n",
    "output_json = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f598146-b9f2-49a1-b58a-04ed9bd4f5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "input_doc = fitz.open(input_file)\n",
    "output_doc = None\n",
    "if output_pdf:\n",
    "    output_doc = fitz.open()\n",
    "\n",
    "results = []\n",
    "zoom = 2    # zoom factor\n",
    "mat = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "for page_no, page in enumerate(input_doc):\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    page_results = parse_image(td_model, device, image)\n",
    "    page_results = apply_ocr(ocr_model, ocr_processor, image, page_results, device)\n",
    "    results.append(page_results)\n",
    "\n",
    "    if output_pdf:\n",
    "        out_page = output_doc.new_page(width=page.rect.width, height=page.rect.height)\n",
    "        out_page.show_pdf_page(page.rect, input_doc, page_no)\n",
    "        add_text_annotations(out_page, page_results)\n",
    "        \n",
    "    print(\"Processed Page: %d\" % page_no)\n",
    "\n",
    "if output_json:\n",
    "    output_json_file = os.path.join(output_path, basename + '.json')\n",
    "    with open(output_json_file, 'w') as fp:\n",
    "        json.dump(results, fp)\n",
    "\n",
    "if output_pdf:\n",
    "    output_pdf_file = os.path.join(output_path, basename + '.pdf')\n",
    "    output_doc.save(output_pdf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
